{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_total = pd.read_csv(\"../leo_kaggle/data.csv\")\n",
    "df_total;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25697\n"
     ]
    }
   ],
   "source": [
    "df_total[\"months\"] = df_total[\"game_date\"].apply(lambda x:x.split('-')[1])\n",
    "df_total[\"years\"] = df_total[\"game_date\"].apply(lambda x:x.split('-')[0])\n",
    "df_trans = df_total[df_total[\"shot_made_flag\"] * 1 == df_total[\"shot_made_flag\"]]\n",
    "# We take only a tenth of the set to play with to have a better speed\n",
    "\n",
    "df = df_trans.iloc[:len(df_trans)]\n",
    "\n",
    "df_test = df_total[df_total[\"shot_made_flag\"] * 1 != df_total[\"shot_made_flag\"]]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cartesian_to_polar(df, name_x, name_y):\n",
    "    df[\"loc_r\"]=df.apply(lambda row:np.sqrt(row[name_x]**2 + row[name_y]**2), axis =1)\n",
    "    df[\"loc_teta\"]= df.apply(lambda row:np.arccos(row[name_x] / row[\"loc_r\"]) if row[\"loc_r\"] else 0, axis=1)\n",
    "\n",
    "def drop_and_binarize(df, to_binarize, to_drop):\n",
    "    new_df = df.copy()\n",
    "    to_binarize =[feature for feature in to_binarize if feature not in to_drop]\n",
    "    print(len(new_df.columns))\n",
    "    new_df.drop(to_drop, axis=1, inplace=True)\n",
    "    print(len(new_df.columns))\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    for feature in to_binarize:\n",
    "        lb.fit(df[feature])\n",
    "        new_df[feature]=df[feature].apply(lambda x:lb.transform([x]).flatten())\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aux_df_to_array(ar):\n",
    "    l = list()\n",
    "    for elem in ar:\n",
    "            if type(elem) is numpy.ndarray:\n",
    "                l.extend(elem)\n",
    "            else:\n",
    "                l.append(elem)\n",
    "    return np.array(l)\n",
    "\n",
    "def df_to_array(df):\n",
    "    return np.apply_along_axis(aux_df_to_array, 1, df.values)\n",
    "\n",
    "def indice_to_name(df):\n",
    "    return np.array([[key for i in range(len(np.unique(df[key])))] for key in df.columns]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale(vect_train, min_max):\n",
    "    res = preprocessing.MinMaxScaler(min_max)\n",
    "    return res.fit_transform(vect_train)\n",
    "\n",
    "def standardize(vect_train):\n",
    "    sta = preprocessing.StandardScaler()\n",
    "    return sta.fit_transform(vect_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_shots_before(shot):\n",
    "    game_id = shot[\"game_id\"]\n",
    "    shots = df[df[\"game_id\"] == game_id]\n",
    "    mask = (shots[\"minutes_remaining\"] == shot[\"minutes_remaining\"]) & (shots[\"seconds_remaining\"] > shot[\"seconds_remaining\"]) | (shots[\"minutes_remaining\"] > shot[\"minutes_remaining\"])\n",
    "    if shot[\"shot_id\"]%1000 ==0:\n",
    "        print(shot[\"shot_id\"])\n",
    "    return shots[mask][\"shot_made_flag\"].count(), shots[mask][\"shot_made_flag\"].sum()\n",
    "\n",
    "def n_successive_shots(shot):\n",
    "    game_id = shot[\"game_id\"]\n",
    "    shots = df[df[\"game_id\"] == game_id]\n",
    "    shots = shots.sort_values(by=[\"minutes_remaining\", \"seconds_remaining\"], ascending=False).reset_index()\n",
    "    #On trouve l'index de shot\n",
    "    index = 0\n",
    "    for i, a in enumerate(shots[\"shot_id\"]==shot[\"shot_id\"]):\n",
    "        if a:\n",
    "            index = i\n",
    "    result = 0\n",
    "    for i in range(index - 1, -1, -1):\n",
    "        if not shots.iloc[i][\"shot_made_flag\"] :\n",
    "            result = index - i -1\n",
    "            break\n",
    "        elif not i :\n",
    "            result = index\n",
    "    return result\n",
    "\n",
    "def add_on_fire_features(df):\n",
    "    col = df.apply(n_shots_before, axis =1)\n",
    "    col2 = df.apply(n_successive_shots, axis =1)\n",
    "    #df[\"n_successive_shots\"] = col2.reset_index(drop=True)\n",
    "    #df[\"n_shots_tried_before\"]=col.apply(lambda x:x[0]).reset_index(drop=True)\n",
    "    #df[\"n_shots_made_before\"]=col.apply(lambda x:x[1]).reset_index(drop=True)\n",
    "    #df[\"previous_win_frequency\"]=col.apply(lambda x: (x[1] / x[0]) if x[0]>0 else 0 ).reset_index(drop=True)\n",
    "    df[\"n_successive_shots\"] = col2\n",
    "    df[\"n_shots_tried_before\"]=col.apply(lambda x:x[0])\n",
    "    df[\"n_shots_made_before\"]=col.apply(lambda x:x[1])\n",
    "    df[\"previous_win_frequency\"]=col.apply(lambda x: (x[1] / x[0]) if x[0]>0 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "cartesian_to_polar(df, \"loc_x\", \"loc_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_on_fire_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_binarize = [\"action_type\", \"combined_shot_type\", \"shot_type\", \"shot_zone_area\", \"shot_zone_basic\", \"shot_zone_range\", \"team_name\", \"matchup\", \"opponent\", \"months\", \"years\", \"team_id\", \"game_event_id\", \"season\"]\n",
    "#features_to_drop = [\"playoffs\", \"team_id\", \"team_name\", \"opponent\", \"lat\", \"lon\", \"season\", \"shot_zone_basic\", \"shot_zone_range\", \"shot_distance\"]\n",
    "features_to_drop = [ \"team_name\", \"shot_id\", \"game_date\", \"game_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "new_df = drop_and_binarize(df, features_to_binarize, features_to_drop)\n",
    "#new_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop([\"game_event_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop([\"matchup\"], axis=1, inplace=True)\n",
    "new_df.drop([\"team_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "vect_df_target = new_df[\"shot_made_flag\"].values\n",
    "new_df = drop_and_binarize(new_df, [], [\"shot_made_flag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_extended(thing):\n",
    "    try:\n",
    "        return len(thing)\n",
    "    except TypeError:\n",
    "        return 1\n",
    "    \n",
    "def indice_to_name(df):\n",
    "    l = list()\n",
    "    for key in df.columns:\n",
    "        l.extend([key for i in range(len_extended(df[key][1]))])\n",
    "    return np.array(l)\n",
    "traducteur = indice_to_name(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_df_features = df_to_array(new_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20557, 174)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "vect_train_features, vect_test_features, vect_train_target, vect_test_target = train_test_split(vect_df_features, vect_df_target, test_size=0.2, shuffle=True)\n",
    "print(vect_train_features.shape)\n",
    "vect_train_features = standardize(vect_train_features)\n",
    "vect_test_features = standardize(vect_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20557, 174), (20557,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_train_features.shape, vect_train_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vect_train_features = vect_df_features[: int(len(vect_df_features) * 0.8)]\n",
    "vect_test_features = vect_df_features[int(len(vect_df_features) * 0.8) :]\n",
    "vect_train_target = vect_df_target[: int(len(vect_df_features) * 0.8)]\n",
    "vect_test_target = vect_df_target[int(len(vect_df_features)*0.8):]\n",
    "vect_train_features = standardize(vect_train_features)\n",
    "vect_test_features = standardize(vect_test_features)\n",
    "#vect_train_features = rescale(vect_train_features, (0,1))\n",
    "#vect_test_features = rescale(vect_test_features, (0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect_train_features, vect_test_features = vect_test_features, vect_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vect_train_target, vect_test_target = vect_test_target, vect_train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's feed it to our nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m, i):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(i)\n",
    "\n",
    "def init_weight_normal(neuron, mean, std):\n",
    "    if type(neuron) == nn.Linear:\n",
    "        neuron.weight.data.normal_(mean, std)\n",
    "\n",
    "def init_weight_relu(neuron):\n",
    "    if type(neuron) == nn.Linear:\n",
    "        n = len(neuron.weight.data)\n",
    "        neuron.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "        \n",
    "def init_normalized_bengio(neuron):\n",
    "    if type(neuron) == nn.Linear:\n",
    "        n_output, n_input = neuron.weight.data.numpy().shape\n",
    "        neuron.weight.data.uniform_(0, np.sqrt(6 / (n_input + n_output)))\n",
    "        \n",
    "def init_uniform(neuron):\n",
    "    if type(neuron) == nn.Linear:\n",
    "        n_output, n_input = neuron.weight.data.numpy().shape\n",
    "        neuron.weight.data.uniform_(0, np.sqrt(1 / (n_input)))\n",
    "        \n",
    "def normalize(vect):\n",
    "    vect = (vect - np.mean(vect)) / np.std(vect)\n",
    "    return vect\n",
    "\n",
    "    \n",
    "def weight_scaling_inference_rule(net, p_init, p_hidden):\n",
    "    \"\"\"Multiply the weight of each layer by its probability of not being droppped out during training to average bagging\"\"\"\n",
    "    list(net.children())[0].weight.data *= (1-p_init)\n",
    "    for neuron in list(net.children())[1:]:\n",
    "        if neuron.type == nn.Linear:\n",
    "            neuron.weight.data *= (1-p_hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net4(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes, dropout=(0, 0)):\n",
    "        super(Net4, self).__init__()\n",
    "        self.p_init = dropout[0]\n",
    "        self.p_hidden = dropout[1]\n",
    "        self.fc1 = nn.Linear(n_features, 20, True)\n",
    "        self.fc2 = nn.Linear(20, 20, True)\n",
    "        self.fc3 = nn.Linear(20, 20, True)\n",
    "        self.fc4 = nn.Linear(20, 20, True)\n",
    "        self.fc5 = nn.Linear(20, 10, True)\n",
    "        self.fc6 = nn.Linear(10, 20, True)\n",
    "        self.fc7 = nn.Linear(20, 20, True)\n",
    "        self.fc11 = nn.Linear(20, 20, True)\n",
    "        self.fc12 = nn.Linear(20, 20, True)\n",
    "        self.fc13 = nn.Linear(20, n_classes, True)\n",
    "        self.fc14 = nn.LogSoftmax()\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(F.dropout(self.fc1(x), p=self.p_init, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc2(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc3(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc4(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc5(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc6(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc7(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc11(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc12(x), p=self.p_hidden, training=self.training))\n",
    "        x = self.fc13(x)\n",
    "        x = self.fc14(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net5(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes, dropout=(0, 0)):\n",
    "        super(Net5, self).__init__()\n",
    "        self.p_init = dropout[0]\n",
    "        self.p_hidden = dropout[1]\n",
    "        self.fc1 = nn.Linear(n_features, 20, True)\n",
    "        self.fc2 = nn.Linear(20, 20, True)\n",
    "        self.fc3 = nn.Linear(20, 20, True)\n",
    "        self.fc4 = nn.Linear(20, 20, True)\n",
    "        self.fc13 = nn.Linear(20, n_classes, True)\n",
    "        self.fc14 = nn.LogSoftmax()\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(F.dropout(self.fc1(x), p=self.p_init, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc2(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc3(x), p=self.p_hidden, training=self.training))\n",
    "        x = F.relu(F.dropout(self.fc4(x), p=self.p_hidden, training=self.training))\n",
    "        x = self.fc13(x)\n",
    "        x = self.fc14(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import copy\n",
    "\n",
    "class MultiInitialisationClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Sklearn-compatible classifier that implement a pytorch feedforward neural network \n",
    "    and tries several weight initialisation in its \"fit\" method\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, net_class=nn.Sequential, criterion=nn.NLLLoss(), batch_size=32, n_epochs=10, \n",
    "                 n_inits_per_optimizer=15, max_fail=4, dropout = (0, 0), init_function=init_weight_relu, validation_set=None,\n",
    "                 optimizer_classes={\"Adadelta\":functools.partial(optim.Adadelta, weight_decay=0.01)}):\n",
    "        \n",
    "        self.trained_net = None\n",
    "        self.dropout = dropout\n",
    "        self.p_init = dropout[0]\n",
    "        self.p_hidden = dropout[1]\n",
    "        self.criterion = criterion\n",
    "        self.net_class = net_class\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_inits_per_optimizer= n_inits_per_optimizer\n",
    "        self.max_fail= max_fail\n",
    "        self.optimizer_classes = optimizer_classes\n",
    "        self.init_function = init_function\n",
    "        self.validation_set = validation_set\n",
    "    \n",
    "    def multi_init(self):\n",
    "        net_list = list()\n",
    "        for i, key in enumerate(self.optimizer_classes.keys()):\n",
    "            for k in range(self.n_inits_per_optimizer):\n",
    "                net = self.net_class_init()       \n",
    "                net.apply(self.init_function)\n",
    "                #net.apply(init_uniform)\n",
    "                net.train()\n",
    "                net_list.append([net, self.optimizer_classes[key](net.parameters()), 0.0])\n",
    "        return net_list\n",
    "    \n",
    "    def load_data(self, dataset):\n",
    "        trainloader = DataLoader(dataset, batch_size = self.batch_size, shuffle=True)\n",
    "        return trainloader\n",
    "    \n",
    "    def fit(self, X, y, n_epochs=None,\n",
    "            validation_set=None, max_fail=5, thresold_fail=1.5):\n",
    "        \n",
    "        print(self.dropout)\n",
    "        print(self.batch_size)\n",
    "        print(self.optimizer_classes)\n",
    "        self.p_init = self.dropout[0]\n",
    "        self.p_hidden = self.dropout[1]\n",
    "        self.net_class_init=functools.partial(self.net_class, n_features=X.shape[1], n_classes=len(np.unique(y)),dropout=self.dropout)\n",
    "        if n_epochs == None:\n",
    "            n_epochs = self.n_epochs\n",
    "        if validation_set == None:\n",
    "            validation_set = self.validation_set\n",
    "\n",
    "        ## Load the data\n",
    "        input, target = torch.from_numpy(X), torch.from_numpy(y)\n",
    "        dataset = TensorDataset(input, target)\n",
    "        trainloader = self.load_data(dataset)\n",
    "        \n",
    "        ## Create the different initialisation (i.e create different nets)\n",
    "        net_list = self.multi_init()\n",
    "\n",
    "\n",
    "        ## Train the nets\n",
    "        print(\"BEGINNING TRAINING\")\n",
    "        loss_list = list()\n",
    "        loss_list_validation = list()\n",
    "        best_net_on_validation = [None, np.inf]\n",
    "        for epoch in range(n_epochs):\n",
    "            for net in net_list :\n",
    "                net[2] = 0.0\n",
    "                n_iter = 0\n",
    "            for data in trainloader:\n",
    "                input, target = data\n",
    "                target = target.view(-1)\n",
    "                input, target = Variable(input).float(), Variable(target).long()\n",
    "                for i in range(len(net_list)):\n",
    "                    net, optimizer, _ = net_list[i]\n",
    "                    optimizer.zero_grad()\n",
    "                    output = net(input)\n",
    "                    loss = self.criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    net_list[i][2] += loss.data[0]\n",
    "                n_iter += 1\n",
    "            index_min = np.nanargmin([net[2] for net in net_list])\n",
    "            print(net_list[index_min][2] / n_iter)\n",
    "            loss_list.append(net_list[index_min][2] / n_iter)\n",
    "            ## Check the loss on the validation set and stop if it's increasing for at least max_fail epochs \n",
    "            #  or go back above thresold_fail * (minimum loss attained on the validation set)\n",
    "            if validation_set:\n",
    "                # copy the best net and make it in evaluation mode for scoring\n",
    "                best_net = copy.deepcopy(net_list[index_min][0])\n",
    "                weight_scaling_inference_rule(best_net, self.p_init, self.p_hidden)\n",
    "                best_net.eval()\n",
    "                score = self.score(validation_set[0], validation_set[1], best_net)\n",
    "                print(score)\n",
    "                loss_list_validation.append(score)\n",
    "                if score < best_net_on_validation[1]: #WARNING : loss or score ???\n",
    "                    best_net_on_validation = [best_net, score]\n",
    "                if (score > thresold_fail * best_net_on_validation[1] or\n",
    "                    max_fail < len(loss_list_validation) \n",
    "                    and (np.array([loss_list_validation[i] - loss_list_validation[i-1] for i in range(- 1, - max_fail - 1, - 1)]) > 0).all()\n",
    "                    ) :\n",
    "                    print(\"EARLY STOPPING\")\n",
    "                    self.trained_net = best_net_on_validation[0]\n",
    "                    return loss_list, loss_list_validation\n",
    "            ##\n",
    "            \n",
    "            if len(net_list) > 1:\n",
    "                del net_list[np.argmax([net[2] for net in net_list])]\n",
    "            print(epoch)\n",
    "            \n",
    "        self.trained_net = best_net_on_validation[0]\n",
    "        l = loss_list, loss_list_validation #before\n",
    "        return self #for sklearn\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.trained_net:\n",
    "            raise RuntimeError(\"You must train the classifier before predicting !\")\n",
    "        previous_state = self.trained_net.training\n",
    "        self.trained_net.eval()\n",
    "        input = Variable(torch.from_numpy(X)).float()\n",
    "        output = self.trained_net(input)\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        self.trained_net.training = previous_state\n",
    "        return predicted.numpy()\n",
    "    \n",
    "        \n",
    "    def predict_proba(self, X, net=None):\n",
    "        if not net and  not self.trained_net:\n",
    "            raise RuntimeError(\"You must train the classifier before predicting !\")\n",
    "        if not net:\n",
    "            net = self.trained_net\n",
    "        previous_state = net.training\n",
    "        net.eval()\n",
    "        input = Variable(torch.from_numpy(X)).float()\n",
    "        output = net(input)\n",
    "        predicted = output.data\n",
    "        net.training = previous_state\n",
    "        return predicted.numpy()\n",
    "    \n",
    "    def score(self, X, y, net=None):\n",
    "        if not net and  not self.trained_net:\n",
    "            raise RuntimeError(\"You must train the classifier before scoring !\")\n",
    "        if not net:\n",
    "            net = self.trained_net\n",
    "        previous_state = net.training\n",
    "        net.eval()\n",
    "        input, target = Variable(torch.from_numpy(X)).float(), Variable(torch.from_numpy(y)).long()\n",
    "        output = net(input)\n",
    "        net.training = previous_state\n",
    "        return self.criterion(output, target).data[0]\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 0.5)\n",
      "512\n",
      "{'Adadelta': <functools.partial object at 0x7f67f9022cb0>}\n",
      "BEGINNING TRAINING\n",
      "3.99268066011\n",
      "0.74940431118\n",
      "0\n",
      "1.38419383328\n",
      "0.689600110054\n",
      "1\n",
      "0.881436833521\n",
      "0.68711566925\n",
      "2\n",
      "0.737447772084\n",
      "0.687489151955\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3cf9a1e99159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'optimizer_classes = { #\"SGD lr=0.1 momentum=0.9\":functools.partial(optim.SGD, lr=1, momentum=0.9, weight_decay=0.01), \\n                         #\"Adam lr=0.1\":functools.partial(optim.Adam, lr=0.1, weight_decay=0.01), \\n                         \"Adadelta\":functools.partial(optim.Adadelta, weight_decay=0.01),\\n                         #\"RMSprop lr=0.02\":functools.partial(optim.RMSprop, lr=1.2, weight_decay=0.01)\\n                        }\\nclf = MultiInitialisationClassifier(Net5, nn.NLLLoss(), dropout=(0.2, 0.5), optimizer_classes=optimizer_classes, n_epochs = 30 , n_inits_per_optimizer = 40, batch_size=512,\\n       validation_set=[vect_test_features,vect_test_target], max_fail = 4)\\nl = clf.fit(vect_train_features, vect_train_target, max_fail = 5, thresold_fail = 1.5)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-234578d7d4e1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_epochs, validation_set, max_fail, thresold_fail)\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mnet_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/thnn/auto.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mupdate_grad_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_grad_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mgi_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_without_bias\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mupdate_grad_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgi_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mgrad_input_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer_classes = { #\"SGD lr=0.1 momentum=0.9\":functools.partial(optim.SGD, lr=1, momentum=0.9, weight_decay=0.01), \n",
    "                         #\"Adam lr=0.1\":functools.partial(optim.Adam, lr=0.1, weight_decay=0.01), \n",
    "                         \"Adadelta\":functools.partial(optim.Adadelta, weight_decay=0.01),\n",
    "                         #\"RMSprop lr=0.02\":functools.partial(optim.RMSprop, lr=1.2, weight_decay=0.01)\n",
    "                        }\n",
    "clf = MultiInitialisationClassifier(Net5, nn.NLLLoss(), dropout=(0.2, 0.5), optimizer_classes=optimizer_classes, n_epochs = 30 , n_inits_per_optimizer = 40, batch_size=512,\n",
    "       validation_set=[vect_test_features,vect_test_target], max_fail = 4)\n",
    "l = clf.fit(vect_train_features, vect_train_target, max_fail = 5, thresold_fail = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must train the classifier before predicting !",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-71af95de6569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect_test_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect_test_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-234578d7d4e1>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, net)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_net\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must train the classifier before predicting !\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must train the classifier before predicting !"
     ]
    }
   ],
   "source": [
    "output0, target0 = clf.predict_proba(vect_test_features), vect_test_target\n",
    "output = Variable(torch.from_numpy(output0)).float()\n",
    "target = Variable(torch.from_numpy(target0)).long()\n",
    "criterion = nn.NLLLoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output0, target0 = clf.predict_proba(vect_train_features), vect_train_target\n",
    "output = Variable(torch.from_numpy(output0)).float()\n",
    "target = Variable(torch.from_numpy(target0)).long()\n",
    "criterion = nn.NLLLoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(vect_train_features).reshape(-1)\n",
    "expected = vect_train_target\n",
    "print( \"{} correct points for {} points : accuracy = {}\".format(sum(predicted==expected), len(predicted), float(sum(predicted==expected))/ len(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(vect_test_features).reshape(-1)\n",
    "expected = vect_test_target\n",
    "print( \"{} correct points for {} points : accuracy = {}\".format(sum(predicted==expected), len(predicted), float(sum(predicted==expected))/ len(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss_list = l[0]\n",
    "test_loss_list = l[1]\n",
    "print(min(train_loss_list))\n",
    "print(min(test_loss_list))\n",
    "epochs = [i for i in range(len(train_loss_list))]\n",
    "f, ax = plt.subplots(2)\n",
    "f.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "ax[0].set_title(\"train loss\")\n",
    "ax[0].plot(epochs, train_loss_list)\n",
    "ax[1].set_title(\"test_loss\")\n",
    "ax[1].plot(epochs, test_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.argsort(np.sum(np.abs(clf.trained_net.fc1.weight.data.numpy()), axis=0))[::-1]\n",
    "#c =  np.argsort(np.sum(np.abs(clf.trained_net.fc1.bias.data.numpy() + np.abs(clf.trained_net.fc1.weight.data.numpy()), axis=0))[::-1])\n",
    "print(traducteur[a])\n",
    "#print(traducteur[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeling and grid-searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay_list = [ 0.01, 0.1]\n",
    "dropout_list = [(0, 0), (0.2, 0.5), (0, 0.5), (0.2, 0.7), (0, 0.7)]\n",
    "#n_inits_per_optimizer_list = [12, 50]\n",
    "batch_size_list = [8, 128, 512, 2048]\n",
    "optimizer_classes_list = [{\"Adadelta\":functools.partial(optim.Adadelta, weight_decay=i)} for i in weight_decay_list]\n",
    "optimizer_classes_list.append({\"Adam\":functools.partial(optim.SGD, lr=3, weight_decay=0.01)})\n",
    "optimizer_classes_list.append({\"SGD\":functools.partial(optim.SGD, lr=2.5, momentum=0.99, weight_decay=0.01)})\n",
    "\n",
    "print(len(optimizer_classes_list))\n",
    "print(len(dropout_list))\n",
    "print( len(optimizer_classes_list) * len(batch_size_list)  * len(dropout_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "optimizer_classes = { \"SGD lr=0.1 momentum=0.9\":functools.partial(optim.SGD, lr=0.2, momentum=0.9), \n",
    "                         \"Adam lr=0.1\":functools.partial(optim.Adam, lr=0.2), \n",
    "                         \"Adadelta\":functools.partial(optim.Adadelta),\n",
    "                         \"RMSprop lr=0.02\":functools.partial(optim.RMSprop, lr=0.05)\n",
    "                        }\n",
    "\n",
    "clf = MultiInitialisationClassifier(Net5, nn.NLLLoss())\n",
    "pipe = Pipeline([(\"preprocessing\", StandardScaler), (\"optimizer_classes\", svm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters = {'optimizer_classes':optimizer_classes_list, 'dropout':dropout_list, 'batch_size':batch_size_list}\n",
    "clf = MultiInitialisationClassifier(net_class=Net5, criterion=nn.NLLLoss(), n_epochs = 40 , n_inits_per_optimizer = 30)\n",
    "clf = GridSearchCV(clf, parameters, cv=2, n_jobs=-1)\n",
    "clf.fit(vect_df_features, vect_df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'optimizer_classes':optimizer_classes_list, 'dropout':dropout_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
