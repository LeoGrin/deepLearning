{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import cPickle\n",
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Toy Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12)\n"
     ]
    }
   ],
   "source": [
    "alphabet = [\"(\", \")\"]\n",
    "def random_sentence():\n",
    "    sentence = \"(\"\n",
    "    depth = 1\n",
    "    for j in range(500):\n",
    "        if depth == 0:\n",
    "            if np.random.binomial(1, 0.8) == 1:\n",
    "                sentence += \"(\"\n",
    "                depth += 1\n",
    "            else:\n",
    "                sentence += \"E\"\n",
    "                break\n",
    "        elif depth > 0:\n",
    "            if np.random.binomial(1, 0.4) == 1:\n",
    "                sentence += \"(\"\n",
    "                depth +=1\n",
    "            else:\n",
    "                sentence += \")\"\n",
    "                depth -= 1\n",
    "    return sentence\n",
    "\n",
    "random_sentence()\n",
    "\n",
    "def trainingExample():\n",
    "    text = random_sentence()\n",
    "    return Variable(text2input(text[:-1])), Variable(text2target(text))\n",
    "\n",
    "\n",
    "def text2input(text):\n",
    "    dict_trad = {\"(\" : 0, \")\":1, \"E\":2}\n",
    "    train_vec = torch.zeros(len(text) , 1, 3)\n",
    "    for i, char in enumerate(text):\n",
    "        train_vec[i][0][dict_trad[char]] = 1\n",
    "\n",
    "        \n",
    "    return train_vec\n",
    "\n",
    "def text2target(text):\n",
    "    dict_trad = {\"(\" : 0, \")\":1, \"E\":2}\n",
    "    target_list = [dict_trad[char] for char in text[1:]]\n",
    "    return torch.LongTensor(target_list)\n",
    "\n",
    "a = trainingExample()\n",
    "print(len(a[0]), len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_correct(text):\n",
    "    if text[-1] != \"E\":\n",
    "        print(\"No EOS\")\n",
    "        return False\n",
    "    else:\n",
    "        max_depth = 0\n",
    "        depth = 0\n",
    "        for char in text[:-1]:\n",
    "            if char == \"(\":\n",
    "                depth +=1\n",
    "            elif char == \")\":\n",
    "                depth -= 1\n",
    "            else:\n",
    "                print(\"Unknow character\")\n",
    "                return False\n",
    "            if depth < 0:\n",
    "                print(\"Negative depth\")\n",
    "                return False\n",
    "            if depth > max_depth:\n",
    "                max_depth = depth\n",
    "    \n",
    "        if depth == 0:\n",
    "            print(\"Maximum depth : {}\".format(max_depth))\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Maximum depth : {}\".format(max_depth))\n",
    "            print(\"Missing {} closing brakets\".format(depth))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from model import RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        #self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, dropout=0.1)\n",
    "        self.lin = nn.Linear(hidden_size, output_size)\n",
    "        #self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        #self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        #self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #hidden = self.i2h(input_combined)\n",
    "        #output = self.i2o(input_combined)\n",
    "        #output_combined = torch.cat((hidden, output), 1)\n",
    "        output, hidden = self.lstm(input.view(len(input), 1, -1), hidden)\n",
    "        output = self.lin(output.view(1, -1))\n",
    "        #output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (Variable(torch.zeros(self.n_layers, 1, self.hidden_size)),\n",
    "                Variable(torch.zeros(self.n_layers, 1, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN (\n",
       "  (lstm): LSTM(3, 20, dropout=0.1)\n",
       "  (lin): Linear (20 -> 3)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = (3, 20, 3)\n",
    "\n",
    "saved_to = open(\"rnn_toy_language_torch\", \"rb\")\n",
    "\n",
    "rnn = RNN(tab[0], tab[1], tab[2])\n",
    "\n",
    "rnn.load_state_dict(torch.load(saved_to))\n",
    "\n",
    "rnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving saved states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192359\n"
     ]
    }
   ],
   "source": [
    "read_hidden_state_file = open(\"/pickle2/hidden_state_file_toy_language\", \"r\")\n",
    "\n",
    "\n",
    "brute_list = read_hidden_state_file.readlines()\n",
    "\n",
    "picklised_tuple_list = list()\n",
    "current_string = \"\"\n",
    "for line in brute_list:\n",
    "    if line != \"NEW_TUPLE\\n\":\n",
    "        current_string += line\n",
    "    else:\n",
    "        picklised_tuple_list.append(current_string)\n",
    "        current_string = \"\"\n",
    "\n",
    "input_size = 3\n",
    "hidden_size = 20\n",
    "\n",
    "tuple_list = [cPickle.loads(a) for a in picklised_tuple_list[1:]]\n",
    "print(len(tuple_list))\n",
    "state_list = [(Variable(torch.from_numpy(tuple[:input_size]).view(1, 1, -1)), (Variable(torch.from_numpy(tuple[input_size: input_size + hidden_size]).view(1, 1, -1)), Variable(torch.from_numpy(tuple[input_size + hidden_size:]).view(1, 1, -1)))) for tuple in tuple_list]\n",
    "\n",
    "output_list = [state[:input_size] for state in tuple_list]\n",
    "\n",
    "output_matrix = np.matrix(output_list)\n",
    "\n",
    "read_hidden_state_file.close()\n",
    "\n",
    "brute_list = 0, 0\n",
    "\n",
    "picklised_tuple_list = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_distance_between_states(tuple_list, input_size, distance, n_iters=1000):\n",
    "    l_all = list()\n",
    "    l_output = list()\n",
    "    l_hidden = list()\n",
    "    nb_hidden_inf_min = 0\n",
    "    nb_output_inf_min = 0\n",
    "    n_both = 0\n",
    "    for i in range(n_iters):\n",
    "        index1 = int(np.random.random() * (len(tuple_list) - 1))\n",
    "        index2 = int(np.random.random() * (len(tuple_list) - 1))\n",
    "        l_all.append(distance(tuple_list[index1], tuple_list[index2]))\n",
    "        d_output = distance(tuple_list[index1][:input_size], tuple_list[index2][:input_size])\n",
    "        if d_output > 0:\n",
    "            l_output.append(d_output)\n",
    "        d_hidden = distance(tuple_list[index1][input_size:], tuple_list[index2][input_size:])\n",
    "        if d_hidden > 0:\n",
    "            l_hidden.append(d_hidden)\n",
    "        if d_hidden < 0.5:\n",
    "            nb_hidden_inf_min += 1\n",
    "        if d_output < 1E-3:\n",
    "            nb_output_inf_min += 1\n",
    "        if d_hidden < 0.5 and d_output < 1E-3:\n",
    "            n_both += 1\n",
    "        if d_hidden == 0:\n",
    "            print(\"hidden\")\n",
    "        if d_output == 0:\n",
    "            print(\"output\")\n",
    "    print(\"means\")\n",
    "    print(np.mean(l_all), np.mean(l_output), np.mean(l_hidden))\n",
    "    print(\"std\")\n",
    "    print(np.std(l_all), np.std(l_output), np.std(l_hidden))\n",
    "    print(\"max\")\n",
    "    print(np.max(l_all), np.max(l_output), np.max(l_hidden))\n",
    "    print(\"min\")\n",
    "    print(np.min(l_all), np.min(l_output), np.min(l_hidden))\n",
    "    print(nb_output_inf_min , nb_hidden_inf_min, n_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means\n",
      "(8.0055669665052118, 0.21050451613499907, 7.9958041511291205)\n",
      "std\n",
      "(5.8772480498122039, 0.28331196535164876, 5.8799392216231672)\n",
      "max\n",
      "(48.198883558521494, 0.98629855753994533, 48.190545378422833)\n",
      "min\n",
      "(0.0059229443408932462, 1.430872736563283e-05, 0.0059227772021624974)\n",
      "(441, 803, 47)\n",
      "CPU times: user 7.06 s, sys: 80 ms, total: 7.14 s\n",
      "Wall time: 7.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mean_distance_between_states(tuple_list, 3, euclidian_distance, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def hellinger(p, q):\n",
    "    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q))**2)) / np.sqrt(2)\n",
    "\n",
    "def KL_divergence(p, q):\n",
    "    return np.sum(p * np.log(p / q.astype(float)))\n",
    "\n",
    "def JS_divergence(p, q):\n",
    "    \"\"\" turn KL divergence into a metric (symetric and triangular inequality)\"\"\"\n",
    "    m = (p + q) / float(2)\n",
    "    return 0.5 * KL_divergence(p, m) + 0.5 * KL_divergence(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidian_distance(state1, state2):\n",
    "    \"\"\"on vectorized states\"\"\"\n",
    "    return (torch.from_numpy(state1).float() - torch.from_numpy(state2).float()).norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_euclidian_distance(state1, state2):\n",
    "    epsilon_array = np.array([1E-10 for _ in range(len(state1))])\n",
    "    state1 = np.log(state1 + epsilon_array)\n",
    "    state2 = np.log(state2 + epsilon_array)\n",
    "    return euclidian_distance(state1, state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(tuple_list, n_clusters_output, size_clusters_hidden):\n",
    "    \"\"\"the network \"states\" to be clustered are composed of a probability distribution (for the next character)\n",
    "    and of a memory state (hidden and cell state). We first cluster states based on their probability distrib\n",
    "    and then we subcluster each cluster obtained according to their memory state\"\"\"\n",
    "    \n",
    "    state_list = [(Variable(torch.from_numpy(tuple[:input_size]).view(1, 1, -1)), (Variable(torch.from_numpy(tuple[input_size: input_size + hidden_size]).view(1, 1, -1)), Variable(torch.from_numpy(tuple[input_size + hidden_size:]).view(1, 1, -1)))) for tuple in tuple_list]\n",
    "    output_list = [state[:input_size] for state in tuple_list]\n",
    "    output_matrix = np.matrix(output_list)\n",
    "    # Cluster according to output\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters = n_clusters_output, n_jobs=-1).fit(output_matrix) \n",
    "    list_indices_first_clusters = [[i for i in range(len(tuple_list)) if kmeans.labels_[i]==cluster] for cluster in np.unique(kmeans.labels_)]\n",
    "    hidden_states_per_output_cluster_list = [[tuple_list[i][input_size:] for i in cluster] for cluster in list_indices_first_clusters]\n",
    "    #list_indices_final_clusters = list() #save in which cluster are each original state (mostly to check clustering correctness)\n",
    "    #list_centers_final_clusters = list() #save the cluster centers (i.e the states of the finite automaton)\n",
    "    list_centers_first_clusters = kmeans.cluster_centers_\n",
    "    list_subclusters_centers = list()\n",
    "    # Subcluster according to memory \n",
    "    for i, output_cluster in enumerate(hidden_states_per_output_cluster_list):\n",
    "        matrix = np.matrix(output_cluster)\n",
    "        kmeans_i = sklearn.cluster.KMeans(n_clusters = max(len(output_cluster) / size_clusters_hidden, 1), n_jobs=-1).fit(matrix)\n",
    "        list_subclusters_centers.append(kmeans_i.cluster_centers_)\n",
    "        #list_indices_hidden_cluster_in_output_cluster = [[j for j in range(len(output_cluster)) if kmeans_i.labels_[j]==cluster] for cluster in np.unique(kmeans_i.labels_)]\n",
    "        #list_indices_final_clusters.extend([[list_indices_first_clusters[i][j] for j in hidden_cluster] for hidden_cluster in list_indices_hidden_cluster_in_output_cluster])\n",
    "        #output_cluster_centers = [np.mean([tuple_list[list_indices_first_clusters[i][k]][:input_size] for k in list_indices_hidden_cluster_in_output_cluster[l]], axis=0) for l in range(len(list_indices_hidden_cluster_in_output_cluster))]\n",
    "        #list_centers_final_clusters.extend([(output_cluster_centers[k], kmeans_i.cluster_centers_[k]) for k in range(len(output_cluster_centers))])\n",
    "    return  list_centers_first_clusters, list_subclusters_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batch_from_database(file_to_read, batch_size):\n",
    "    \"\"\"\n",
    "    Get a batch from the file_to_read (in the order of the file)\n",
    "    Returns : a sklearn_compatible matrix of the batch\n",
    "    \"\"\"\n",
    "    batch = list()\n",
    "    for _ in range(batch_size):\n",
    "        picklised_example = \"\"\n",
    "        for i in range(1000):\n",
    "            picklised_line = file_to_read.readline()\n",
    "            if picklised_line == \"\":\n",
    "                print(\"END OF FILE\")\n",
    "                raise EOFError\n",
    "                break\n",
    "            if picklised_line != \"NEW_TUPLE\\n\":\n",
    "                picklised_example += picklised_line\n",
    "            else:\n",
    "                batch.append(picklised_example)\n",
    "                break\n",
    "    batch = np.matrix([cPickle.loads(a) for a in batch])\n",
    "    return batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator_nb_clusters_mini_batch_kmeans(kmeans_estimator, file_to_read, n_clusters_output, batch_size, n_batchs, input_size):\n",
    "    \"\"\"\n",
    "    estimate the density of the output clusters\n",
    "    \"\"\"\n",
    "    clusters_density = np.zeros(n_clusters_output)\n",
    "    kmeans = kmeans_estimator\n",
    "    for i in range(n_batchs):\n",
    "        batch_matrix = get_mini_batch_from_database(file_to_read, batch_size)\n",
    "        output_batch_matrix = batch_matrix[:, : 3]\n",
    "        kmeans.partial_fit(output_batch_matrix)\n",
    "        labels = kmeans.labels_\n",
    "        for k in range(n_clusters_output):\n",
    "            clusters_density[k] += sum(labels == k)\n",
    "    return clusters_density / float(n_batchs * batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_to_read = open(\"/pickle2/hidden_state_file_toy_language\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batch_kmeans_clustering(filename, n_clusters_output, size_clusters_hidden, batch_size, n_batchs, input_size, hidden_size):\n",
    "    \"\"\"\n",
    "    Cluster the saved states of the network streamingly through the MiniBatchKmeans algorithm.\n",
    "    The file_to_read should be randomized first.\n",
    "    \"\"\"\n",
    "    # we have to specify the number of subclusters in each output cluster, but we cannot store \n",
    "    # the members of each cluster, and we cannot store their indices (states are only available as streams)\n",
    "    # so we estimate the size of each output cluster to compute the size if the subclusters\n",
    "    kmeans_output = sklearn.cluster.MiniBatchKMeans(n_clusters_output, batch_size=batch_size)\n",
    "    print(\"estimating subcluster size\")\n",
    "    file_to_read = open(filename, \"r\")\n",
    "    cluster_density = estimator_nb_clusters_mini_batch_kmeans(kmeans_output, file_to_read, n_clusters_output, batch_size, 50, input_size) # TODO : n_batchs ?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "    print(\"estimation over, initiating clustering\")\n",
    "    file_to_read = open(filename, \"r\") # reset readline\n",
    "    subclusters_size = (cluster_density * batch_size * n_batchs / size_clusters_hidden).astype(int)\n",
    "    print(subclusters_size)\n",
    "    kmeans_subclusters = [sklearn.cluster.MiniBatchKMeans(subclusters_size[i], batch_size=batch_size) for i in range(n_clusters_output)] # TODO : change batch size with subcluster size\n",
    "    subclusters_batches = [list() for _ in range(n_clusters_output)] # store the hidden_vector for each cluster while waiting for subcluster\n",
    "    for i in range(n_batchs):\n",
    "        try:\n",
    "            batch_matrix = get_mini_batch_from_database(file_to_read, batch_size)\n",
    "        except EOFError:\n",
    "            break\n",
    "        output_batch_matrix = batch_matrix[:, : input_size]\n",
    "        hidden_batch_matrix = batch_matrix[:, input_size:]\n",
    "        kmeans_output.partial_fit(output_batch_matrix)\n",
    "        labels = kmeans_output.labels_\n",
    "        for cluster in range(n_clusters_output):\n",
    "            for j in range(batch_size):\n",
    "                if labels[j] == cluster:\n",
    "                    subclusters_batches[cluster].append(np.array(hidden_batch_matrix[j])[0])\n",
    "        for k, subcluster_batch in enumerate(subclusters_batches):\n",
    "            if len(subcluster_batch) >= batch_size:\n",
    "                subcluster_matrix = np.array(subcluster_batch)\n",
    "                subclusters_batches[k] = list()\n",
    "                kmeans_subclusters[k].partial_fit(subcluster_matrix)\n",
    "        \n",
    "    return kmeans_output.cluster_centers_, [kmeans.cluster_centers_ for kmeans in kmeans_subclusters]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimating subcluster size\n",
      "estimation over, initiating clustering\n",
      "[27  9  3  2  1  9 26  9 14  1]\n",
      "END OF FILE\n",
      "CPU times: user 17.7 s, sys: 36 ms, total: 17.7 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename = \"/pickle2/hidden_state_file_toy_language\"\n",
    "list_centers_first_clusters, list_subclusters_centers = mini_batch_kmeans_clustering(filename, 10, 10000, 300, 3500, 3, 40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 4.74 s, total: 25.5 s\n",
      "Wall time: 54.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_centers_first_clusters, list_subclusters_centers = kmeans_clustering(tuple_list, 10, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(a) for a in list_subclusters_centers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_subclusters_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(a) for a in list_subclusters_centers]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.12901253e-01   5.85594296e-01   1.50380284e-03]\n",
      " [  8.10950279e-01   1.37669109e-02   1.75282404e-01]\n",
      " [  3.44245911e-01   6.54674530e-01   1.07952754e-03]\n",
      " [  5.28011739e-01   4.44499761e-01   2.74883285e-02]\n",
      " [  6.25752747e-01   3.01131785e-01   7.31153563e-02]\n",
      " [  3.84899676e-01   6.13780260e-01   1.32000411e-03]\n",
      " [  2.93613851e-01   7.05602169e-01   7.83839962e-04]\n",
      " [  4.53209281e-01   5.43887377e-01   2.90350337e-03]\n",
      " [  7.74459839e-01   6.23917580e-02   1.63148463e-01]\n",
      " [  7.15725541e-01   1.58109263e-01   1.26165524e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(list_centers_first_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the automaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_loss_rnn(rnn, n_examples):\n",
    "    \"\"\"\n",
    "    return the mean of the log_loss of the RNN when trying to predict the \n",
    "    next character of random correct sentences\n",
    "    \"\"\"\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    nb_loss = 0\n",
    "    rnn.eval()\n",
    "    for i in range(n_examples):\n",
    "        text = random_sentence()\n",
    "        target = Variable(text2target(text))\n",
    "        input = Variable(text2input(text[:-1]))\n",
    "        hidden = rnn.initHidden()\n",
    "        for i in range(len(input)):\n",
    "            output, hidden = rnn(input[i], hidden)\n",
    "            output = F.log_softmax(output)\n",
    "            total_loss += criterion(output, target[i])\n",
    "            nb_loss += 1\n",
    "    return total_loss / float(nb_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_loss_automata(automata, n_examples):\n",
    "    \"\"\"\n",
    "    return the mean of the log_loss of the automata when trying to predict the \n",
    "    next character of random correct sentences\n",
    "    \"\"\"\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    nb_loss = 0\n",
    "    #print(list(automata.neighbors(-1)))\n",
    "    for i in range(n_examples):\n",
    "        text = random_sentence()\n",
    "        target = Variable(text2target(text)) # RNN don't predict the first \"(\" so we don't make the automata predict it so we can compare results\n",
    "        current_node = move_state_automata(automata, -1, \"(\")        \n",
    "        for k, char in enumerate(text[1:]):\n",
    "            predicted_distrib = Variable(torch.from_numpy(return_proba_distrib(automata, current_node))).log() #log proba to use NLLLoss\n",
    "            total_loss += criterion( predicted_distrib, target[k])\n",
    "            nb_loss += 1\n",
    "            current_node = move_state_automata(automata, current_node, char)\n",
    "    return total_loss / float(nb_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_state_automata(automata, node, char):\n",
    "    \"\"\"\n",
    "    auxiliary function to move the automaton from one state to the next after one character (return the next node)\n",
    "    \"\"\"\n",
    "    neighbors = list(automata.neighbors(node))\n",
    "    for neighbor in neighbors:\n",
    "        if char in [d[\"label\"] for d in automata.get_edge_data(node, neighbor).values()]:\n",
    "            return neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_proba_distrib(automata, node):\n",
    "    \"\"\"\n",
    "    return the proba distrib for the next character for a given automaton state\n",
    "    \"\"\"\n",
    "    distrib_proba = list()\n",
    "    neighbors = list(automata.neighbors(node))\n",
    "    for char in char_vect:\n",
    "        for neighbor in neighbors:\n",
    "            for key in automata.get_edge_data(node, neighbor).keys():\n",
    "                if automata.get_edge_data(node, neighbor)[key][\"label\"] == char:\n",
    "                    distrib_proba.append(automata.get_edge_data(node, neighbor)[key][\"weight\"])\n",
    "    return np.array(distrib_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_automata(automata, start_node=-1, temperature=1, print_nb_visited_nodes=False):\n",
    "    \"\"\"\n",
    "    Generate words according to the automaton transitions probability\n",
    "    \"\"\"\n",
    "    visited_nodes = list()\n",
    "    result=\"\"\n",
    "    current_node = start_node\n",
    "    for i in range(5000):\n",
    "        if current_node not in visited_nodes:\n",
    "            visited_nodes.append(current_node)\n",
    "        neighbors = list(automata.neighbors(current_node))\n",
    "        attrib_list = [automata.get_edge_data(current_node, neighbor)for neighbor in neighbors]\n",
    "        label_list = np.concatenate([np.array([attrib[key][\"label\"] for key in attrib.keys()])for attrib in attrib_list])\n",
    "        proba_list = np.concatenate([np.array([attrib[key][\"weight\"] for key in attrib.keys()]) for attrib in attrib_list])\n",
    "        proba_list /= sum(proba_list)\n",
    "        next_index = np.where(np.random.multinomial(1, proba_list))[0][0]\n",
    "        #next_index = np.argmax(proba_list)\n",
    "        result += label_list[next_index]\n",
    "        if label_list[next_index] == \"E\":\n",
    "            if print_nb_visited_nodes:\n",
    "                print(\"nb visited nodes : {}\".format(len(visited_nodes)))\n",
    "            return result\n",
    "        else:\n",
    "            s = 0\n",
    "            for k in range(len(neighbors)):\n",
    "                s += len(attrib_list[k].keys())\n",
    "                if next_index < s:\n",
    "                    current_node = neighbors[k]\n",
    "                    break\n",
    "    if print_nb_visited_nodes:\n",
    "        print(\"nb visited nodes : {}\".format(len(visited_nodes)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "automata2 = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_subclusters_centers)):\n",
    "    for j in range(len(list_subclusters_centers[i])):\n",
    "        automata2.add_node((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "automata2.add_node(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_vect = ['(', ')', 'E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to(vect, list_vect, distance=euclidian_distance):\n",
    "    return np.argmin([distance(vect, vect2) for vect2 in list_vect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_closest(vect, list_vect, n_closest, distance=euclidian_distance):\n",
    "    distance_list = [distance(vect, vect2) for vect2 in list_vect]\n",
    "    indices_closest = np.argsort(distance_list)[:n_closest]\n",
    "    distances_closest = np.array(distance_list)[indices_closest]\n",
    "    return indices_closest, distances_closest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(automata2.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transition_closest(automata, rnn, start_node, start_output, start_hidden, list_subclusters_centers, list_first_clusters_centers,n_rec=0):\n",
    "    \"\"\"\n",
    "    Create the transitions between the finite automaton states according to the rnn.\n",
    "    (the rnn tells from wich memory state to which memory state we should go after a character).\n",
    "    \"\"\"\n",
    "    n_rec +=1\n",
    "    if n_rec > 7000:\n",
    "        return\n",
    "    result_list = list()\n",
    "\n",
    "    for i, proba in enumerate(start_output):\n",
    "        char_as_vect = Variable(text2input(char_vect[i]))\n",
    "        hidden_state = (Variable(torch.from_numpy(start_hidden[:hidden_size]).view(1, 1, -1)).float(), Variable(torch.from_numpy(start_hidden[hidden_size:]).view(1, 1, -1).float()))\n",
    "        output, new_hidden = rnn(char_as_vect, hidden_state)\n",
    "        output = F.softmax(output)\n",
    "        output = output.data.numpy()\n",
    "        new_hidden = np.concatenate((new_hidden[0].data.numpy(), new_hidden[1].data.numpy()))\n",
    "        index_closest_output = closest_to(output, list_first_clusters_centers, euclidian_distance)\n",
    "        index_closest_hidden = closest_to(new_hidden, list_subclusters_centers[index_closest_output], euclidian_distance)\n",
    "        cluster_output = list_first_clusters_centers[index_closest_output]\n",
    "        cluster_hidden = list_subclusters_centers[index_closest_output][index_closest_hidden]\n",
    "        new_node = (index_closest_output, index_closest_hidden)\n",
    "        add_node = True\n",
    "        data = automata.get_edge_data(start_node, new_node)\n",
    "        if data:\n",
    "            if char_vect[i] in [d[\"label\"] for d in data.values()]:\n",
    "                add_node = False\n",
    "        \n",
    "        if add_node:\n",
    "            automata.add_edge(start_node, new_node, label=char_vect[i], weight=float(proba))\n",
    "            add_transition_closest(automata, rnn, new_node, cluster_output, cluster_hidden, list_subclusters_centers, list_first_clusters_centers, n_rec)\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_transition_closest(automata2, rnn, -1, np.array((1., 0, 0)), np.zeros(40).astype(float), list_subclusters_centers, list_centers_first_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 432\n",
      "Number of edges: 1065\n",
      "Average in degree:   2.4653\n",
      "Average out degree:   2.4653\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(automata2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [len(list(automata2.neighbors((node)))) for node in automata2.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(l==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb visited nodes : 3\n",
      "longueur : 3\n",
      "()E\n",
      "Maximum depth : 1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "res = generate_from_automata(automata2, print_nb_visited_nodes=True)\n",
    "print(\"longueur : {}\".format(len(res)))\n",
    "print(res)\n",
    "print(is_correct(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.75 s, sys: 188 ms, total: 8.94 s\n",
      "Wall time: 8.94 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6570\n",
       "[torch.DoubleTensor of size 1]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "compute_log_loss_automata(automata2, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 1s, sys: 420 ms, total: 5min 2s\n",
      "Wall time: 14.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6608\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "compute_log_loss_rnn(rnn, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_parallel_edges_gephi_compatible(automata, filename):\n",
    "    new_automata = nx.DiGraph()\n",
    "    for node in automata.nodes():\n",
    "        new_automata.add_node(node)\n",
    "    for node in automata.nodes():\n",
    "        neighbors = automata.neighbors(node)\n",
    "        for neighbor in neighbors:\n",
    "            data = automata.get_edge_data(node, neighbor)\n",
    "            weight = sum([v[\"weight\"] for v in data.values()])\n",
    "            label = \"\".join([v[\"label\"] + \" : \" + str(v[\"weight\"]) for v in data.values()])\n",
    "            new_automata.add_edge(node, neighbor, label=label, weight=weight)\n",
    "    print(nx.info(new_automata))\n",
    "    nx.write_graphml(new_automata, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 19\n",
      "Number of edges: 18\n",
      "Average in degree:   0.9474\n",
      "Average out degree:   0.9474\n"
     ]
    }
   ],
   "source": [
    "export_parallel_edges_gephi_compatible(automata2, \"automata_0.4.graphml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
